#!/usr/bin/env python3
"""
check_ollama.py - Script para verificar y configurar Ollama
"""

import subprocess
import json
import sys
import requests
import time

def check_ollama_running():
    """Verificar si Ollama estÃ¡ ejecutÃ¡ndose"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def start_ollama():
    """Intentar iniciar Ollama"""
    try:
        if sys.platform == "win32":
            subprocess.Popen(["ollama", "serve"], shell=True)
        else:
            subprocess.Popen(["ollama", "serve"])
        
        print("ðŸ”„ Iniciando Ollama...")
        time.sleep(5)
        return check_ollama_running()
    except:
        return False

def check_models():
    """Verificar modelos disponibles"""
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json()
            return [model['name'] for model in models.get('models', [])]
        return []
    except:
        return []

def pull_model(model_name):
    """Descargar modelo si no existe"""
    try:
        print(f"ðŸ“¥ Descargando modelo {model_name}...")
        result = subprocess.run(
            ["ollama", "pull", model_name], 
            capture_output=True, text=True
        )
        return result.returncode == 0
    except:
        return False

def main():
    print("ðŸ” Verificando configuraciÃ³n de Ollama...")
    
    # 1. Verificar si Ollama estÃ¡ corriendo
    if not check_ollama_running():
        print("âŒ Ollama no estÃ¡ ejecutÃ¡ndose")
        if start_ollama():
            print("âœ… Ollama iniciado correctamente")
        else:
            print("âŒ No se pudo iniciar Ollama")
            print("ðŸ’¡ Instala Ollama desde: https://ollama.ai")
            return False
    else:
        print("âœ… Ollama estÃ¡ ejecutÃ¡ndose")
    
    # 2. Verificar modelos
    models = check_models()
    print(f"ðŸ“¦ Modelos disponibles: {len(models)}")
    
    required_models = [
        "llama3.2:latest",
        "qwen2.5-coder:7b"
    ]
    
    # 3. Descargar modelos necesarios
    for model in required_models:
        if not any(model in m for m in models):
            print(f"â¬‡ï¸ Descargando {model}...")
            if pull_model(model):
                print(f"âœ… {model} descargado")
            else:
                print(f"âŒ Error descargando {model}")
        else:
            print(f"âœ… {model} ya disponible")
    
    # 4. Verificar final
    final_models = check_models()
    print(f"\nðŸ“‹ Modelos finales disponibles: {len(final_models)}")
    for model in final_models:
        print(f"   â€¢ {model}")
    
    return len(final_models) > 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)